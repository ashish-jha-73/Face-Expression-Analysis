{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36ad7986-beed-4e72-964e-cb392a354ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28822 files belonging to 7 classes.\n",
      "Found 7066 files belonging to 7 classes.\n",
      "Detected classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:10:54.773879: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-08 02:10:57.038696: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_3156', 520 bytes spill stores, 520 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.2605 - loss: 2.1243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-08 02:13:01.259455: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-11-08 02:13:16.047645: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:310] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.19GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 147ms/step - accuracy: 0.3201 - loss: 1.8496 - val_accuracy: 0.4108 - val_loss: 1.5272\n",
      "Epoch 2/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 133ms/step - accuracy: 0.4515 - loss: 1.4297 - val_accuracy: 0.4742 - val_loss: 1.3781\n",
      "Epoch 3/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 141ms/step - accuracy: 0.4973 - loss: 1.3224 - val_accuracy: 0.4060 - val_loss: 1.5791\n",
      "Epoch 4/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 142ms/step - accuracy: 0.5175 - loss: 1.2690 - val_accuracy: 0.5207 - val_loss: 1.2581\n",
      "Epoch 5/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 144ms/step - accuracy: 0.5353 - loss: 1.2258 - val_accuracy: 0.5306 - val_loss: 1.2341\n",
      "Epoch 6/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 140ms/step - accuracy: 0.5615 - loss: 1.1707 - val_accuracy: 0.5395 - val_loss: 1.2144\n",
      "Epoch 7/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 141ms/step - accuracy: 0.5767 - loss: 1.1344 - val_accuracy: 0.5303 - val_loss: 1.2320\n",
      "Epoch 8/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 141ms/step - accuracy: 0.5903 - loss: 1.0898 - val_accuracy: 0.5524 - val_loss: 1.1963\n",
      "Epoch 9/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 143ms/step - accuracy: 0.6001 - loss: 1.0635 - val_accuracy: 0.5562 - val_loss: 1.1745\n",
      "Epoch 10/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 146ms/step - accuracy: 0.6190 - loss: 1.0188 - val_accuracy: 0.5185 - val_loss: 1.3303\n",
      "Epoch 11/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 144ms/step - accuracy: 0.6329 - loss: 0.9807 - val_accuracy: 0.5818 - val_loss: 1.1270\n",
      "Epoch 12/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 145ms/step - accuracy: 0.6472 - loss: 0.9485 - val_accuracy: 0.5562 - val_loss: 1.1747\n",
      "Epoch 13/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 144ms/step - accuracy: 0.6602 - loss: 0.9172 - val_accuracy: 0.5780 - val_loss: 1.1333\n",
      "Epoch 14/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 135ms/step - accuracy: 0.6739 - loss: 0.8798 - val_accuracy: 0.5858 - val_loss: 1.1281\n",
      "Epoch 15/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 146ms/step - accuracy: 0.6816 - loss: 0.8553 - val_accuracy: 0.5604 - val_loss: 1.2040\n",
      "Epoch 16/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 145ms/step - accuracy: 0.6929 - loss: 0.8302 - val_accuracy: 0.5876 - val_loss: 1.1391\n",
      "Epoch 17/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 129ms/step - accuracy: 0.7021 - loss: 0.7962 - val_accuracy: 0.5914 - val_loss: 1.1333\n",
      "Epoch 18/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 94ms/step - accuracy: 0.7107 - loss: 0.7789 - val_accuracy: 0.5812 - val_loss: 1.1361\n",
      "Epoch 19/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 94ms/step - accuracy: 0.7196 - loss: 0.7567 - val_accuracy: 0.5594 - val_loss: 1.2016\n",
      "Epoch 20/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 95ms/step - accuracy: 0.7273 - loss: 0.7392 - val_accuracy: 0.5909 - val_loss: 1.1246\n",
      "Epoch 21/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 94ms/step - accuracy: 0.7371 - loss: 0.7140 - val_accuracy: 0.5735 - val_loss: 1.2231\n",
      "Epoch 22/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 94ms/step - accuracy: 0.7450 - loss: 0.6895 - val_accuracy: 0.5859 - val_loss: 1.1859\n",
      "Epoch 23/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 96ms/step - accuracy: 0.7557 - loss: 0.6695 - val_accuracy: 0.5788 - val_loss: 1.2301\n",
      "Epoch 24/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 91ms/step - accuracy: 0.7548 - loss: 0.6599 - val_accuracy: 0.5817 - val_loss: 1.2205\n",
      "Epoch 25/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7636 - loss: 0.6421 - val_accuracy: 0.5798 - val_loss: 1.2031\n",
      "Epoch 26/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7775 - loss: 0.6163 - val_accuracy: 0.5907 - val_loss: 1.1924\n",
      "Epoch 27/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7741 - loss: 0.6131 - val_accuracy: 0.5825 - val_loss: 1.2427\n",
      "Epoch 28/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7816 - loss: 0.5982 - val_accuracy: 0.5889 - val_loss: 1.1921\n",
      "Epoch 29/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7872 - loss: 0.5826 - val_accuracy: 0.5831 - val_loss: 1.2452\n",
      "Epoch 30/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 91ms/step - accuracy: 0.7921 - loss: 0.5669 - val_accuracy: 0.5835 - val_loss: 1.2575\n",
      "Epoch 31/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.7964 - loss: 0.5597 - val_accuracy: 0.5842 - val_loss: 1.2481\n",
      "Epoch 32/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 91ms/step - accuracy: 0.8010 - loss: 0.5402 - val_accuracy: 0.5937 - val_loss: 1.2500\n",
      "Epoch 33/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.8038 - loss: 0.5391 - val_accuracy: 0.5883 - val_loss: 1.2714\n",
      "Epoch 34/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 91ms/step - accuracy: 0.8091 - loss: 0.5244 - val_accuracy: 0.5821 - val_loss: 1.2758\n",
      "Epoch 35/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 91ms/step - accuracy: 0.8112 - loss: 0.5163 - val_accuracy: 0.5904 - val_loss: 1.2867\n",
      "Epoch 36/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.8142 - loss: 0.5101 - val_accuracy: 0.5940 - val_loss: 1.2425\n",
      "Epoch 37/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - accuracy: 0.8198 - loss: 0.4957 - val_accuracy: 0.5886 - val_loss: 1.2847\n",
      "Epoch 38/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 102ms/step - accuracy: 0.8244 - loss: 0.4868 - val_accuracy: 0.5920 - val_loss: 1.2665\n",
      "Epoch 39/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 124ms/step - accuracy: 0.8273 - loss: 0.4800 - val_accuracy: 0.5984 - val_loss: 1.2978\n",
      "Epoch 40/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 117ms/step - accuracy: 0.8290 - loss: 0.4764 - val_accuracy: 0.5934 - val_loss: 1.2518\n",
      "Epoch 41/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 118ms/step - accuracy: 0.8310 - loss: 0.4617 - val_accuracy: 0.5903 - val_loss: 1.3130\n",
      "Epoch 42/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 119ms/step - accuracy: 0.8288 - loss: 0.4595 - val_accuracy: 0.5935 - val_loss: 1.3416\n",
      "Epoch 43/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 120ms/step - accuracy: 0.8377 - loss: 0.4495 - val_accuracy: 0.5957 - val_loss: 1.3382\n",
      "Epoch 44/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 119ms/step - accuracy: 0.8384 - loss: 0.4492 - val_accuracy: 0.5890 - val_loss: 1.3484\n",
      "Epoch 45/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 119ms/step - accuracy: 0.8377 - loss: 0.4446 - val_accuracy: 0.5927 - val_loss: 1.3819\n",
      "Epoch 46/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 120ms/step - accuracy: 0.8424 - loss: 0.4333 - val_accuracy: 0.5887 - val_loss: 1.3601\n",
      "Epoch 47/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 120ms/step - accuracy: 0.8452 - loss: 0.4278 - val_accuracy: 0.5950 - val_loss: 1.3292\n",
      "Epoch 48/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 119ms/step - accuracy: 0.8507 - loss: 0.4113 - val_accuracy: 0.5940 - val_loss: 1.3629\n",
      "Epoch 49/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 123ms/step - accuracy: 0.8493 - loss: 0.4218 - val_accuracy: 0.5931 - val_loss: 1.3817\n",
      "Epoch 50/50\n",
      "\u001b[1m901/901\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 120ms/step - accuracy: 0.8556 - loss: 0.4035 - val_accuracy: 0.5948 - val_loss: 1.3939\n",
      "\u001b[1m221/221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - accuracy: 0.5948 - loss: 1.3939\n",
      "✅ Validation accuracy: 59.48%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# DIP Preprocessing Function (fixed)\n",
    "# ---------------------------\n",
    "def dip_preprocess(image):\n",
    "    image = image.numpy()  # convert Tensor -> numpy\n",
    "\n",
    "    # Convert to uint8 safely\n",
    "    if image.dtype != np.uint8:\n",
    "        if image.max() <= 1.0:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        else:\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "    # Ensure it’s 3-channel RGB\n",
    "    if image.ndim == 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif image.shape[2] == 4:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    # Convert to grayscale (uint8)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Histogram equalization (safe since gray is uint8)\n",
    "    eq = cv2.equalizeHist(gray)\n",
    "\n",
    "    # Canny edge detection\n",
    "    edges = cv2.Canny(eq, 100, 200)\n",
    "\n",
    "    # Overlay edges on equalized image\n",
    "    overlay = cv2.addWeighted(eq, 0.8, edges, 0.2, 0)\n",
    "\n",
    "    # Convert back to 3-channel RGB\n",
    "    overlay = cv2.merge([overlay, overlay, overlay])\n",
    "\n",
    "    # Normalize to [0,1]\n",
    "    overlay = overlay.astype(np.float32) / 255.0\n",
    "    return overlay\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Batch-safe preprocessing\n",
    "# ---------------------------\n",
    "def preprocess_dataset(dataset):\n",
    "    def _map_fn(x, y):\n",
    "        x = tf.map_fn(\n",
    "            lambda img: tf.py_function(func=dip_preprocess, inp=[img], Tout=tf.float32),\n",
    "            x,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        x.set_shape((None, 48, 48, 3))\n",
    "        y = tf.cast(y, tf.int32)\n",
    "        return x, y\n",
    "\n",
    "    return dataset.map(_map_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Load Datasets\n",
    "# ---------------------------\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../images/train\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../images/validation\",\n",
    "    color_mode=\"rgb\",\n",
    "    image_size=(48, 48),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(\"Detected classes:\", class_names)\n",
    "\n",
    "# Apply DIP preprocessing\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ---------------------------\n",
    "# CNN Model\n",
    "# ---------------------------\n",
    "model = keras.Sequential([\n",
    "    layers.Rescaling(1./1, input_shape=(48, 48, 3)),\n",
    "\n",
    "    layers.Conv2D(512, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(256, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.25),\n",
    "\n",
    "    layers.Conv2D(128, (3,3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(2, 2),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Compile and Train\n",
    "# ---------------------------\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(val_ds)\n",
    "print(f\"✅ Validation accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146179f-b819-4012-a9a5-d746ae576d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
